{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae76ebb1-8314-40aa-8920-a70eb15bb9ab",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The data source generates files with a dynamic or varying number of columns, often without headers.\n",
    "\n",
    "**Unstructured Data:** Data that doesn't have a fixed schema, such as logs, machine-generated data, or files with inconsistent columns.\n",
    "\n",
    "**Variable Columns:** Files where different rows may have different numbers of columns, making it challenging to process using standard CSV readers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc9d0c49-9273-4ae3-8b2d-cea22befc159",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 75 bytes.\nOut[1]: True"
     ]
    }
   ],
   "source": [
    "dbutils.fs.put(\"/scenarios/dynamic_withoutheader.csv\", \"\"\"1,ravi\n",
    "2,ram,bangalore\n",
    "3,prasad,chennai,sample@gmail.com,9283923\n",
    "4,Sam,Pune\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5f57299-8ca7-4697-b51f-b3ab09b3bfef",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Initial Attempt: Using csv Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4368dd8b-8815-4d2c-9612-6304e5110fc8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>_c0</th><th>_c1</th></tr></thead><tbody><tr><td>1</td><td>ravi</td></tr><tr><td>2</td><td>ram</td></tr><tr><td>3</td><td>prasad</td></tr><tr><td>4</td><td>Sam</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "1",
         "ravi"
        ],
        [
         "2",
         "ram"
        ],
        [
         "3",
         "prasad"
        ],
        [
         "4",
         "Sam"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "_c0",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "_c1",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = spark.read.csv(\"/scenarios/dynamic_withoutheader.csv\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a988762-b5dd-448c-9a5d-d74a67a3494b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "If you use csv without headers, PySpark will treat the first row as the definition of the number of columns, which could cause issues if the subsequent rows have more or fewer columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84c8cff3-f438-4128-898d-2fd44f603d46",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Solution:Reading as Text File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6c625ef-79b9-49eb-bf81-dcf6238e494a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>value</th></tr></thead><tbody><tr><td>1,ravi</td></tr><tr><td>2,ram,bangalore</td></tr><tr><td>3,prasad,chennai,sample@gmail.com,9283923</td></tr><tr><td>4,Sam,Pune</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "1,ravi"
        ],
        [
         "2,ram,bangalore"
        ],
        [
         "3,prasad,chennai,sample@gmail.com,9283923"
        ],
        [
         "4,Sam,Pune"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "value",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df1 = spark.read.text(\"/scenarios/dynamic_withoutheader.csv\")\n",
    "display(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e098df0-9c3c-40f5-a4d7-c9bfbfbcbd34",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Instead of using csv, read the file as a text file, which will treat the entire row as a single string. This approach prevents data loss and allows you to process the varying number of columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7818939b-062c-4c69-a2cc-006b9d2214ce",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####Splitting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e00ab172-8cdd-4070-a616-d82f1e90bbbe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>splittable_col</th></tr></thead><tbody><tr><td>List(1, ravi)</td></tr><tr><td>List(2, ram, bangalore)</td></tr><tr><td>List(3, prasad, chennai, sample@gmail.com, 9283923)</td></tr><tr><td>List(4, Sam, Pune)</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         [
          "1",
          "ravi"
         ]
        ],
        [
         [
          "2",
          "ram",
          "bangalore"
         ]
        ],
        [
         [
          "3",
          "prasad",
          "chennai",
          "sample@gmail.com",
          "9283923"
         ]
        ],
        [
         [
          "4",
          "Sam",
          "Pune"
         ]
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "splittable_col",
         "type": "{\"type\":\"array\",\"elementType\":\"string\",\"containsNull\":false}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import split\n",
    "df1 = df1.withColumn('splittable_col',split('value',',').alias(\"splittable_col\")).drop('value')\n",
    "display(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d4ed7c7-02b7-4f6a-bb1a-e6d57d75f51a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Used the split function to divide the single string column into an array of values based on a delimiter (comma). This array can then be used to generate the actual columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0765aff3-a256-4f9d-914f-d8e528b4643a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####Determine the Maximum Number of Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80c8cf2d-6a3a-40e0-abf3-3350cc568245",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import size,max\n",
    "max_columns = df1.select(max(size('splittable_col'))).collect()[0][0]\n",
    "print(max_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c922b457-cc5d-432b-8019-ec1e7db9ee37",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Applied the size function to determine the number of elements in each row. Then, found the maximum size to understand how many columns needed to be generated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "676e1805-47c3-49d8-9335-8394584abb01",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####Generating Dynamic Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99c3073e-00b1-4498-8afe-7b20ca223435",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>splittable_col</th><th>col0</th><th>col1</th><th>col2</th><th>col3</th><th>col4</th></tr></thead><tbody><tr><td>List(1, ravi)</td><td>1</td><td>ravi</td><td>null</td><td>null</td><td>null</td></tr><tr><td>List(2, ram, bangalore)</td><td>2</td><td>ram</td><td>bangalore</td><td>null</td><td>null</td></tr><tr><td>List(3, prasad, chennai, sample@gmail.com, 9283923)</td><td>3</td><td>prasad</td><td>chennai</td><td>sample@gmail.com</td><td>9283923</td></tr><tr><td>List(4, Sam, Pune)</td><td>4</td><td>Sam</td><td>Pune</td><td>null</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         [
          "1",
          "ravi"
         ],
         "1",
         "ravi",
         null,
         null,
         null
        ],
        [
         [
          "2",
          "ram",
          "bangalore"
         ],
         "2",
         "ram",
         "bangalore",
         null,
         null
        ],
        [
         [
          "3",
          "prasad",
          "chennai",
          "sample@gmail.com",
          "9283923"
         ],
         "3",
         "prasad",
         "chennai",
         "sample@gmail.com",
         "9283923"
        ],
        [
         [
          "4",
          "Sam",
          "Pune"
         ],
         "4",
         "Sam",
         "Pune",
         null,
         null
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "splittable_col",
         "type": "{\"type\":\"array\",\"elementType\":\"string\",\"containsNull\":false}"
        },
        {
         "metadata": "{}",
         "name": "col0",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "col1",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "col2",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "col3",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "col4",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(max_columns):\n",
    "  df1 = df1.withColumn(\"col\"+str(i), df1['splittable_col'][i])\n",
    "display(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c62ffa7a-6028-451f-a79b-eb08b302df36",
     "showTitle": false,
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1feb411-7219-47c0-a393-427d698f72b5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####Cleaning Up\n",
    "**Drop Unnecessary Columns:** After successfully creating the new columns, you dropped the original single-column data to clean up the DataFrame and retain only the dynamically generated columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc94b564-302b-49a6-a2e1-e7fd6b2a5ad6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>col0</th><th>col1</th><th>col2</th><th>col3</th><th>col4</th></tr></thead><tbody><tr><td>1</td><td>ravi</td><td>null</td><td>null</td><td>null</td></tr><tr><td>2</td><td>ram</td><td>bangalore</td><td>null</td><td>null</td></tr><tr><td>3</td><td>prasad</td><td>chennai</td><td>sample@gmail.com</td><td>9283923</td></tr><tr><td>4</td><td>Sam</td><td>Pune</td><td>null</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "1",
         "ravi",
         null,
         null,
         null
        ],
        [
         "2",
         "ram",
         "bangalore",
         null,
         null
        ],
        [
         "3",
         "prasad",
         "chennai",
         "sample@gmail.com",
         "9283923"
        ],
        [
         "4",
         "Sam",
         "Pune",
         null,
         null
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "col0",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "col1",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "col2",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "col3",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "col4",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df1 = df1.drop('splittable_col')\n",
    "display(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "812d2648-b959-4240-8129-d52d54cd04ee",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "This approach helped you manage unstructured data in PySpark, particularly when dealing with files that had varying numbers of columns, providing a flexible solution for processing such data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88a2690f-7126-4fb2-8a7c-8e6111964eb1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Dynamic Column Handling in PySpark: Processing Unstructured Data with Variable Column Counts",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
